{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Phase analysis for $\\left| \\phi^+ \\rangle \\right.$ - Pair 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import linalg,stats\n",
    "from scipy.optimize import least_squares,minimize\n",
    "#from numpy import random, linalg\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm,ticker,colors,rc,font_manager\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "%matplotlib inline\n",
    "import os\n",
    "try:\n",
    "    fm = font_manager.json_load(os.path.expanduser(\"~/.cache/matplotlib/fontlist-v310.json\"))\n",
    "except:\n",
    "    fm = font_manager.json_load(os.path.expanduser(\"~/.cache/matplotlib/fontlist-v300.json\"))\n",
    "fm.findfont(\"serif\", rebuild_if_missing=False)\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Computer Modern Sans serif']})\n",
    "## for Palatino and other serif fonts use:\n",
    "rc('font',**{'family':'serif','serif':['Computer Modern Roman']})\n",
    "rc('text', usetex=True)\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some experimental parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of qubits\n",
    "n_qbits = 2\n",
    "# laser repetition rate\n",
    "R = 8E7\n",
    "# number of measurements\n",
    "measurements = 2\n",
    "# seconds per line of datafile (measurement time)\n",
    "meas_time = 1\n",
    "# motor angles\n",
    "#mot_ang = [6,7,8]\n",
    "#mot_ang = [-10,-3,2,6,9,11,13,14,15,16]\n",
    "# detector lists [A1 B1 A2 B2]\n",
    "addresses_sin = {'h1':1,'v1':2,'h2':3,'v2':4}\n",
    "#correct for datafiles\n",
    "datafile_list = False\n",
    "#for key in addresses_sin:\n",
    "#    addresses_sin[key] +=1\n",
    "#print(addresses_sin)\n",
    "#folder with measurments\n",
    "fold = './'\n",
    "\n",
    "# function to create list of coincidences of n_qubits\n",
    "def coinc_str_first(n,coinc_list=['h','v']):\n",
    "    outs = ['h','v']\n",
    "    coinc_tmp = []\n",
    "    for i in coinc_list:\n",
    "        for k in outs:\n",
    "            coinc_tmp.append(i+k)\n",
    "    if n==1:\n",
    "        return outs\n",
    "    elif n==2:\n",
    "        return coinc_tmp\n",
    "    else:\n",
    "        return coinc_str_first(n-1,coinc_tmp)\n",
    "    \n",
    "def coinc_str(n):\n",
    "    tmp_str = coinc_str_first(n)\n",
    "    tmp2 = tmp_str.copy()\n",
    "    for i,entry in enumerate(tmp2):\n",
    "        tmp3 = ''\n",
    "        for j,det in enumerate(entry):\n",
    "            tmp3 += det+str(j+1)\n",
    "        tmp_str[i] = tmp3\n",
    "    return tmp_str\n",
    "\n",
    "# angles for each setting in order [QWP,HWP]\n",
    "#settings = [[-15,-30] for i in range(measurements)]\n",
    "# actual settings\n",
    "settings = [[0,0],[0,22.5]]\n",
    "\n",
    "repeat = 1000 #number of monte-carlo generations\n",
    "#m_per_turn = 300 #number of measurements per turn\n",
    "mot_err = 0.2 #motor error in degrees\n",
    "\n",
    "# correct for accidentals? 'theory' or 'no' (or 'exp' for 2 photons)\n",
    "acc_corr = 'theory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QI definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert deg to rad\n",
    "def theta(x):\n",
    "    return x*np.pi/180\n",
    "\n",
    "# Lambda-Half wave plate\n",
    "def HWP(x):\n",
    "    return np.array([[np.cos(2*theta(x)),np.sin(2*theta(x))],[np.sin(2*theta(x)),-np.cos(2*theta(x))]])\n",
    "\n",
    "# QWP\n",
    "def QWP(x):\n",
    "    return np.array([[np.cos(theta(x))**2+1j*np.sin(theta(x))**2,(1-1j)*np.sin(theta(x))*np.cos(theta(x))],\n",
    "            [(1-1j)*np.sin(theta(x))*np.cos(theta(x)),np.sin(theta(x))**2+1j*np.cos(theta(x))**2]])\n",
    "\n",
    "#pauli matrices\n",
    "s0 = np.eye(2)\n",
    "sx = np.array([[0, 1],[ 1, 0]])\n",
    "sy = np.array([[0, -1j],[1j, 0]])\n",
    "sz = np.array([[1, 0],[0, -1]])\n",
    "\n",
    "#basis states\n",
    "h = np.array([1, 0])\n",
    "v = np.array([0, 1])\n",
    "\n",
    "# Generate measurement operator for n qubits as tensor product of the same measurement sigma\n",
    "def meas_multi(n,sigma):\n",
    "    if n==1:\n",
    "        return sigma\n",
    "    else:\n",
    "        return np.kron(meas_multi(n-1,sigma),sigma)\n",
    "\n",
    "# GHZ^n state\n",
    "def GHZ(nqub=6):\n",
    "    ghz = np.zeros((2**nqub,1))\n",
    "    ghz[0],ghz[-1] = 1/np.sqrt(2),1/np.sqrt(2)\n",
    "    return ghz\n",
    "\n",
    "# visibility\n",
    "def visib(setting, data):\n",
    "    if setting==0:\n",
    "        #vis = (data[0]+data[-1])/sum(data)\n",
    "        #actual sigma_Z visibility\n",
    "        vis = (data[0]+data[-1]-sum(data[1:-1]))/sum(data)\n",
    "    elif setting in range(1,len(data)):\n",
    "        sett_str = coinc_str(n_qbits)\n",
    "        data_plus = []\n",
    "        data_minus = []\n",
    "        for i,sett in enumerate(sett_str):\n",
    "            if (sett.count('h')%2 == 0):\n",
    "                data_plus.append(i)\n",
    "            else:\n",
    "                data_minus.append(i)\n",
    "        num = sum([data[i] for i in data_plus]) - sum([data[i] for i in data_minus])\n",
    "        vis = num/sum(data)\n",
    "    else:\n",
    "        print('Wrong setting number!')\n",
    "    return vis\n",
    "\n",
    "def visiX(data):\n",
    "    num = sum([data[i] for i in [0,3,5,6,9,10,12,15]]) - sum([data[i] for i in [1,2,4,7,8,11,13,14]])\n",
    "    vis = num/sum(data)\n",
    "    return vis\n",
    "\n",
    "#file_lenght\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "#file_len(fold+'1_datafile.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define addresses to import, with labels.\n",
    "Also define accidentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Address(add_dict,datafile_list):\n",
    "    addresses = {}\n",
    "    dets = []\n",
    "    for i in range(int(len(add_dict)/2)):\n",
    "        dets.append(dict(itertools.islice(add_dict.items(), 2*i, 2*i+2)))\n",
    "\n",
    "    if len(add_dict)//2 == 6:\n",
    "        # 6-photon addresses\n",
    "        for comb6 in map(dict,itertools.product(*[dets[i].items() for i in range(len(dets))])):\n",
    "            addresses[''.join([key for key in comb6])] = sum([2**(list(comb6.values())[i]-1) for i in range(len(comb6))])\n",
    "            # 5-photon addresses\n",
    "            for comb5 in map(dict,itertools.combinations(comb6.items(),5)):\n",
    "                addresses[''.join([key for key in comb5])] = sum([2**(list(comb5.values())[i]-1) for i in range(len(comb5))])\n",
    "                # 4-photon addresses\n",
    "                for comb4 in map(dict,itertools.combinations(comb5.items(),4)):\n",
    "                    addresses[''.join([key for key in comb4])] = sum([2**(list(comb4.values())[i]-1) for i in range(len(comb4))])\n",
    "                    # 3-photon addresses\n",
    "                    for comb3 in map(dict,itertools.combinations(comb4.items(),3)):\n",
    "                        addresses[''.join([key for key in comb3])] = sum([2**(list(comb3.values())[i]-1) for i in range(len(comb3))])\n",
    "                        # 2-photon addresses\n",
    "                        for comb2 in map(dict,itertools.combinations(comb3.items(),2)):\n",
    "                            addresses[''.join([key for key in comb2])] = sum([2**(list(comb2.values())[i]-1) for i in range(len(comb2))])\n",
    "                            # 1-photon addresses\n",
    "                            for comb1 in map(dict,itertools.combinations(comb2.items(),1)):\n",
    "                                addresses[''.join([key for key in comb1])] = sum([2**(list(comb1.values())[i]-1) for i in range(len(comb1))])\n",
    "        # list of 5-photon accidentals\n",
    "        acc_list = []\n",
    "        for comb in map(dict,itertools.product(*[dets[i].items() for i in range(len(dets))])):\n",
    "            lst = list(map(dict,itertools.combinations(comb.items(),5)))\n",
    "            tmp = []\n",
    "            for el in comb:\n",
    "                for item in lst:\n",
    "                    if el not in item:\n",
    "                        tmp.append([''.join([key for key in item]),el])\n",
    "            acc_list.append(tmp) \n",
    "    \n",
    "    elif len(add_dict)//2 == 4:\n",
    "        # 4-photon addresses\n",
    "        for comb4 in map(dict,itertools.product(*[dets[i].items() for i in range(len(dets))])):\n",
    "            addresses[''.join([key for key in comb4])] = sum([2**(list(comb4.values())[i]-1) for i in range(len(comb4))])\n",
    "            # 3-photon addresses\n",
    "            for comb3 in map(dict,itertools.combinations(comb4.items(),3)):\n",
    "                addresses[''.join([key for key in comb3])] = sum([2**(list(comb3.values())[i]-1) for i in range(len(comb3))])\n",
    "                # 2-photon addresses\n",
    "                for comb2 in map(dict,itertools.combinations(comb3.items(),2)):\n",
    "                    addresses[''.join([key for key in comb2])] = sum([2**(list(comb2.values())[i]-1) for i in range(len(comb2))])\n",
    "                    # 1-photon addresses\n",
    "                    for comb1 in map(dict,itertools.combinations(comb2.items(),1)):\n",
    "                        addresses[''.join([key for key in comb1])] = sum([2**(list(comb1.values())[i]-1) for i in range(len(comb1))])\n",
    "        # list of 3-photon accidentals\n",
    "        acc_list = []\n",
    "        for comb in map(dict,itertools.product(*[dets[i].items() for i in range(len(dets))])):\n",
    "            lst = list(map(dict,itertools.combinations(comb.items(),3)))\n",
    "            tmp = []\n",
    "            for el in comb:\n",
    "                for item in lst:\n",
    "                    if el not in item:\n",
    "                        tmp.append([''.join([key for key in item]),el])\n",
    "            acc_list.append(tmp) \n",
    "            \n",
    "    elif len(add_dict)//2 == 2:\n",
    "        # 2-photon addresses\n",
    "        for comb2 in map(dict,itertools.product(*[dets[i].items() for i in range(len(dets))])):\n",
    "            addresses[''.join([key for key in comb2])] = sum([2**(list(comb2.values())[i]-1) for i in range(len(comb2))])\n",
    "            # 1-photon addresses\n",
    "            for comb1 in map(dict,itertools.combinations(comb2.items(),1)):\n",
    "                addresses[''.join([key for key in comb1])] = sum([2**(list(comb1.values())[i]-1) for i in range(len(comb1))])\n",
    "        # list of 1-photon accidentals\n",
    "        acc_list = []\n",
    "        for comb in map(dict,itertools.product(*[dets[i].items() for i in range(len(dets))])):\n",
    "            lst = list(map(dict,itertools.combinations(comb.items(),1)))\n",
    "            tmp = []\n",
    "            for el in comb:\n",
    "                for item in lst:\n",
    "                    if el not in item:\n",
    "                        tmp.append([''.join([key for key in item]),el])\n",
    "            acc_list.append(tmp) \n",
    "    \n",
    "    # add 2-photon experimental accidentals\n",
    "    for i,comb in enumerate(dets):\n",
    "        addresses[''.join([key for key in comb])] = sum([2**(list(comb.values())[i]-1) for i in range(len(comb))])\n",
    "        \n",
    "    #sort dictionary\n",
    "    addresses = {k: v for k, v in sorted(addresses.items(), key=lambda item: item[1])}\n",
    "    # fill lists of columns to import and their labels\n",
    "    # name colunms\n",
    "    cols_to_imp = []\n",
    "    cols_labels = []\n",
    "\n",
    "    for el in addresses:\n",
    "        cols_labels.append(el)\n",
    "        cols_to_imp.append(addresses[el])\n",
    "\n",
    "    # correct addresses if datafile comes from list\n",
    "    if datafile_list:\n",
    "        for key in addresses:\n",
    "            addresses[key] +=1\n",
    "        cols_to_imp_tmp = cols_to_imp.copy()\n",
    "        for i,el in enumerate(cols_to_imp_tmp):\n",
    "            cols_to_imp[i] +=1\n",
    "    \n",
    "    return [addresses,cols_to_imp,cols_labels,acc_list]\n",
    "\n",
    "#Address(addresses_sin, datafile_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce and clean files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "['./2_da.dat', './1_hv.dat']\n"
     ]
    }
   ],
   "source": [
    "# Reduce datafiles\n",
    "# what's the highest address needed?\n",
    "def reduce_files(addresses_sin, datafile_lst=datafile_list, folder=fold):\n",
    "    max_add = 1\n",
    "    for det in addresses_sin:\n",
    "        max_add += 2**(addresses_sin[det]-1)\n",
    "\n",
    "    print(max_add)\n",
    "    # find all data files\n",
    "    all_dat_files = glob.glob(folder+'*.dat')\n",
    "    dat_files = []\n",
    "    for fil in all_dat_files:\n",
    "        if 'red' not in fil:\n",
    "            dat_files.append(fil)\n",
    "    print(dat_files)\n",
    "\n",
    "    # clean files\n",
    "    # list of measurements?\n",
    "    if datafile_lst:\n",
    "        for file_num,fil in enumerate(dat_files):\n",
    "            with open(fil) as in_f:\n",
    "                # list to contain cleaned data\n",
    "                cl_data = ''\n",
    "                for line in in_f:\n",
    "                    arry = [x for x in line.split()]\n",
    "                    if arry[1]=='0':\n",
    "                        for num in arry[:max_add]:\n",
    "                            cl_data += num\n",
    "                            cl_data += ' '\n",
    "            #            cl_data += arry[max_add]\n",
    "                        cl_data += '\\n'\n",
    "                with open(str(fil)[:-4]+'_red.dat', 'w') as o_f:\n",
    "                    o_f.write(cl_data)\n",
    "        \n",
    "    elif not datafile_lst:\n",
    "        for file_num,fil in enumerate(dat_files):\n",
    "            with open(fil) as in_f:\n",
    "                # list to contain cleaned data\n",
    "                cl_data = ''\n",
    "                for line in in_f:\n",
    "                    arry = [x for x in line.split()]\n",
    "                    if arry[0]=='0':\n",
    "                        for num in arry[:max_add]:\n",
    "                            cl_data += num\n",
    "                            cl_data += ' '\n",
    "            #            cl_data += arry[max_add]\n",
    "                        cl_data += '\\n'\n",
    "                with open(str(fil)[:-4]+'_red.dat', 'w') as o_f:\n",
    "                    o_f.write(cl_data)\n",
    "                    \n",
    "    else:\n",
    "        print(\"Are all datafiles the same (list or not list)?\")\n",
    "\n",
    "reduce_files(addresses_sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./1_hv_red.dat\n",
      "Total 2-photon events: 13917336. Average rate: 100124.72 Hz\n",
      "Avg rate: 100125 Hz\n",
      "1 ./2_da_red.dat\n",
      "Total 2-photon events: 12779485. Average rate: 98303.73 Hz\n",
      "Avg rate: 98304 Hz\n",
      "Overall:\n",
      "2-photon events: 26696821. Average rate: 99214.23 Hz\n"
     ]
    }
   ],
   "source": [
    "def file_list(setting, folder=fold):\n",
    "    # find all data files\n",
    "\n",
    "    dat_files = glob.glob(folder+'*_red.dat')\n",
    "    dat_files = sorted(dat_files, key=lambda item: int(item.split('_')[0][len(folder):])) #sort datafiles\n",
    "\n",
    "    # find out min lenght\n",
    "    \n",
    "    m_per_turn = file_len(dat_files[setting])\n",
    "\n",
    "    return dat_files[setting],m_per_turn\n",
    "\n",
    "def data_import(setting, datafile_lst=datafile_list, folder=fold):\n",
    "# find all data files\n",
    "\n",
    "    dat_file, m_per_turn = file_list(setting,folder)\n",
    "    \n",
    "    #m_per_turn = 4000 # override m_per_turn\n",
    "    skip_rows = 0 # skip initial rows\n",
    "    m_per_turn = m_per_turn-skip_rows\n",
    "    counts_sing_turn = [] #holds the real data\n",
    "    \n",
    "    adds = Address(addresses_sin,datafile_lst)\n",
    "#   df = pd.read_csv(dat_file, sep=' ', header=None, names=Address(addresses_sin)[2], usecols=Address(addresses_sin)[1], nrows=min(meas_len), engine='python')\n",
    "    df = pd.read_csv(dat_file, sep=' ', header=None, names=adds[2], usecols=adds[1], skiprows=skip_rows, nrows=m_per_turn, dtype={'user_id': int})\n",
    "    setting_dict = {}\n",
    "    # loop on turns\n",
    "    for add in adds[0]:\n",
    "        setting_dict[add] = df[add].sum()\n",
    "    counts_sing_turn.append(setting_dict)\n",
    "\n",
    "    print(setting,dat_file)\n",
    "    \n",
    "    return [counts_sing_turn,m_per_turn]\n",
    "\n",
    "# generate monte-carlo data, 'repeat' amount of sets\n",
    "def MC_data(real_data, reps):\n",
    "    \n",
    "    counts = [real_data.copy()] #will hold total counts for each address, for one setting, for each repetition\n",
    "    \n",
    "    if reps<2:\n",
    "        return counts\n",
    "    \n",
    "    for turn in range(reps-1):\n",
    "        new_data_round = [] #will hold new MC-generated data, for one settings\n",
    "#        counter = 0\n",
    "\n",
    "        new_data_dict = {} #will hold new MC-generated data, for one settings\n",
    "\n",
    "        for el in real_data[0]:\n",
    "            new_data_dict[el] = np.random.default_rng().normal(real_data[0][el],np.sqrt(real_data[0][el]))\n",
    "        new_data_round.append(new_data_dict)\n",
    "        counts.append(new_data_round)\n",
    "\n",
    "    return counts\n",
    "\n",
    "#print(len(MC_data(data_import(),reps=3)))\n",
    "\n",
    "def exp_freq(setting, reps, meas_tm=meas_time, datafile_lst=datafile_list, folder=fold):\n",
    "# now create vectors for experimental frequencies\n",
    "    f_v = [] #n-photon counts\n",
    "\n",
    "    data = data_import(setting, datafile_lst, folder)\n",
    "    counts = MC_data(data[0],reps)\n",
    "    print('Lines in datafile: {}'.format(data[1]))\n",
    "    coincs = coinc_str(n_qbits)\n",
    "    \n",
    "    for i,turn in enumerate(counts):\n",
    "        f_v_sing_turn = []\n",
    "        for m_d in counts[i]:\n",
    "            if acc_corr=='theory':\n",
    "                acc_all = Address(addresses_sin, datafile_lst)[3]\n",
    "                acc = []\n",
    "                for row in acc_all:\n",
    "                    tot = 0\n",
    "                    for pair in row:\n",
    "                        tot += m_d[pair[0]]*m_d[pair[1]]\n",
    "                    acc.append(tot/(n_qbits*R*data[1]*meas_tm))\n",
    "                norm = 0\n",
    "                for i,comb in enumerate(coincs):\n",
    "                    norm += max(m_d[comb]-acc[i],0)\n",
    "\n",
    "                for i,comb in enumerate(coincs):\n",
    "                    f_v_sing_turn.append(max((m_d[comb]-acc[i])/norm,0))\n",
    "            elif acc_corr=='exp': #HARD-CODED: only works for 2 photons\n",
    "                hh = m_d['h1h2']-(m_d['h1v1']+m_d['h2v2'])/2\n",
    "                hv = m_d['h1v2']-(m_d['h1v1']+m_d['h2v2'])/2\n",
    "                vh = m_d['v1h2']-(m_d['h1v1']+m_d['h2v2'])/2\n",
    "                vv = m_d['v1v2']-(m_d['h1v1']+m_d['h2v2'])/2\n",
    "                norm = hh + hv + vh + vv\n",
    "                f_v_sing_turn.append(hh/norm)\n",
    "                f_v_sing_turn.append(hv/norm)\n",
    "                f_v_sing_turn.append(vh/norm)\n",
    "                f_v_sing_turn.append(vv/norm)\n",
    "            elif acc_corr=='no':\n",
    "                norm = sum(m_d[ff] for ff in coincs)\n",
    "                #print(norm)\n",
    "                for comb in coincs:\n",
    "                    f_v_sing_turn.append(m_d[comb]/norm)\n",
    "            else: print('Error with understanding if we want to correct for accs or not')\n",
    "        f_v.append(f_v_sing_turn)\n",
    "    return f_v\n",
    "\n",
    "#function to extract total counts and rate per second for a list of coincidences (e.g. 'h1v2v3h4h5h6')\n",
    "def tot_rate(keys, setting, meas_tm=meas_time, datafile_lst=datafile_list, folder=fold):\n",
    "    counts,m_per_turn = data_import(setting, datafile_lst,folder)\n",
    "    \n",
    "    counts_dict = counts[0]\n",
    "    #total\n",
    "    tot = 0\n",
    "    for key in counts_dict:\n",
    "        if ('1' in key) and ('2' in key):\n",
    "            tot += counts_dict[key]\n",
    "            \n",
    "    print('Total 2-photon events: {}. Average rate: {:.2f} Hz'.format(tot,tot/m_per_turn/meas_time))\n",
    "    p_coinc = (counts_dict['h1h2']+counts_dict['h1v2']+counts_dict['v1h2']+counts_dict['v1v2'])/(m_per_turn*meas_time)\n",
    "                                                          \n",
    "    print('Avg rate: {:.0f} Hz'.format(p_coinc))\n",
    "    \n",
    "    for key in keys:\n",
    "        print(key)\n",
    "        print('Total: ', counts_dict[key], '; Rate: ', counts_dict[key]/m_per_turn/meas_time)\n",
    "\n",
    "    return tot,tot/m_per_turn/meas_time,p_coinc\n",
    "\n",
    "#tot_rate([],0)\n",
    "\n",
    "rates = [tot_rate([],i) for i in range(len(settings))]\n",
    "\n",
    "print('Overall:')\n",
    "print('2-photon events: {}. Average rate: {:.2f} Hz'.format(sum([rates[i][0] for i in range(len(rates))]),\n",
    "                                                            sum([rates[i][1] for i in range(len(rates))])/len(rates)))\n",
    "#print('Avg coincs: P1: {:.0f} Hz, P2: {:.0f} Hz, P3: {:.0f} Hz'.format(sum([rates[i][2] for i in range(len(rates))])/len(rates),\n",
    "#                                                                      sum([rates[i][3] for i in range(len(rates))])/len(rates),\n",
    "#                                                                      sum([rates[i][4] for i in range(len(rates))])/len(rates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for statistical and systematic error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistical error analysis\n",
    "def visi_stat(setting, reps, datafile_lst=datafile_list, folder=fold):\n",
    "    visi = [] # contains visibilities for all Poisson generated data-sets\n",
    "\n",
    "    if setting in range(len(settings)):\n",
    "        f_v = exp_freq(setting, reps, meas_time, datafile_lst, folder) # contains all data (first: real data; next: MC generated)\n",
    "        for i in range(reps):\n",
    "            visi.append(visib(setting,f_v[i]))\n",
    "            \n",
    "    else:\n",
    "        print('Wrong setting number!')\n",
    "        \n",
    "    return visi\n",
    "\n",
    "## Systematic error analysis\n",
    "def visi_syst(setting, reps, mot_err):\n",
    "    visi = [] # contains visibilities for all Poisson generated data-sets\n",
    "\n",
    "    if setting in range(len(settings)):\n",
    "        for i in range(reps):\n",
    "            qwp = np.random.normal(settings[setting][0],mot_err)\n",
    "            hwp = np.random.normal(settings[setting][1],mot_err)\n",
    "            state = meas_multi(n_qbits,QWP(qwp)@HWP(hwp))@GHZ(n_qbits)\n",
    "            #state = meas_multi(n_qbits,QWP(qwp)@HWP(hwp))@state1\n",
    "            visi.append(visib(setting,np.real((state*state.conj()).T[0])))\n",
    "            \n",
    "    else:\n",
    "        print('Wrong setting number!')\n",
    "        \n",
    "    return visi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the different measurement settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./1_hv_red.dat\n",
      "Lines in datafile: 139\n",
      "1 ./2_da_red.dat\n",
      "Lines in datafile: 130\n",
      "Visibilities:\n",
      "[0.9965549997929035, 0.9965558819948128]\n",
      "Montecarlo sampling:\n",
      "Visibilities (MC) - 1000 reps:\n",
      "[0.996554524506782, 0.996555241728712]\n",
      "Statistical errors:\n",
      "- [0.00010755413585683549, 0.00011322067975305661]\n",
      "+ [0.00010787531800271477, 0.0001135378464864889]\n",
      "Systematic errors:\n",
      "- [0.00013622091591858343, 0.00015426248298355816]\n",
      "+ [3.74028424035e-05, 4.1162956170737175e-05]\n"
     ]
    }
   ],
   "source": [
    "visi = [] # will contain median visibility of all settings\n",
    "visi_MC = [] # will contain median visibility of all settings\n",
    "visi_err_stat = [] # will contain statistical errors, negative first row, positive second row\n",
    "visi_err_syst = [] # will contain systematic errors, negative first row, positive second row\n",
    "\n",
    "st_err_neg_tmp = []\n",
    "st_err_pos_tmp = []\n",
    "sy_err_neg_tmp = []\n",
    "sy_err_pos_tmp = []\n",
    "# repeat for each setting\n",
    "for i in range(len(settings)):\n",
    "    vis_tmp = visi_stat(i,repeat,datafile_list,fold)\n",
    "    visi.append(vis_tmp[0])\n",
    "    visi_MC.append(np.median(vis_tmp))\n",
    "    #statistical error\n",
    "    st_err_neg_tmp.append(np.median(vis_tmp)-np.percentile(vis_tmp,50-34.1))\n",
    "    st_err_pos_tmp.append(np.percentile(vis_tmp,50+34.1)-np.median(vis_tmp))\n",
    "    #systematic error\n",
    "    vis_sys_tmp = visi_syst(i,repeat,mot_err)\n",
    "    sy_err_neg_tmp.append(np.median(vis_sys_tmp)-np.percentile(vis_sys_tmp,50-34.1))\n",
    "    sy_err_pos_tmp.append(np.percentile(vis_sys_tmp,50+34.1)-np.median(vis_sys_tmp))\n",
    "\n",
    "visi_err_stat.append(st_err_neg_tmp)\n",
    "visi_err_stat.append(st_err_pos_tmp)\n",
    "visi_err_syst.append(sy_err_neg_tmp)\n",
    "visi_err_syst.append(sy_err_pos_tmp)\n",
    "    \n",
    "print('Visibilities:')\n",
    "print(visi)\n",
    "print('Montecarlo sampling:')\n",
    "print('Visibilities (MC) - {} reps:'.format(repeat))\n",
    "print(visi_MC)\n",
    "print('Statistical errors:')\n",
    "print('-',visi_err_stat[0])\n",
    "print('+',visi_err_stat[1])\n",
    "\n",
    "print('Systematic errors:')\n",
    "print('-',visi_err_syst[0])\n",
    "print('+',visi_err_syst[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\gamma_3^2$ - $\\left| \\phi^+ \\rangle \\right.$ - Fidelity results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity:  0.9974166909456323\n",
      "Statistical errors:\n",
      "- 6.267155526776453e-05\n",
      "+ 6.284925147393608e-05\n",
      "Systematic errors:\n",
      "- 8.431480910800459e-05\n",
      "+ 2.2606039655804895e-05\n"
     ]
    }
   ],
   "source": [
    "#visi[0] = 0.912\n",
    "fidelity = (1 + visi[0] + 2*visi[1])/(2*n_qbits)\n",
    "#statistical\n",
    "fid_stat_err_neg = np.sqrt(visi_err_stat[0][0]**2/16 + visi_err_stat[0][1]**2/4)\n",
    "fid_stat_err_pos = np.sqrt(visi_err_stat[1][0]**2/16 + visi_err_stat[1][1]**2/4)\n",
    "#systematic\n",
    "fid_syst_err_neg = np.sqrt(visi_err_syst[0][0]**2/16 + visi_err_syst[0][1]**2/4)\n",
    "fid_syst_err_pos = np.sqrt(visi_err_syst[1][0]**2/16 + visi_err_syst[1][1]**2/4)\n",
    "\n",
    "print('Fidelity: ',fidelity)\n",
    "print('Statistical errors:')\n",
    "print('-',fid_stat_err_neg)\n",
    "print('+',fid_stat_err_pos)\n",
    "\n",
    "print('Systematic errors:')\n",
    "print('-',fid_syst_err_neg)\n",
    "print('+',fid_syst_err_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visi = [0.9965549997929035, 0.9965558819948128]\n",
    "visi_MC = [0.996554524506782, 0.996555241728712]\n",
    "visi_err_stat = [[0.00010755413585683549, 0.00011322067975305661],[0.00010787531800271477, 0.0001135378464864889]]\n",
    "visi_err_syst = [[0.00013622091591858343, 0.00015426248298355816],[3.74028424035e-05, 4.1162956170737175e-05]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
